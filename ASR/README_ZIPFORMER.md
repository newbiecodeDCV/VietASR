# Zipformer-30M-RNNT Demo v√† Fine-tuning

Scripts cho vi·ªác demo v√† fine-tune model Zipformer-30M-RNNT-6000h t·ª´ HuggingFace.

## üìã T·ªïng quan

Model: [hynt/Zipformer-30M-RNNT-6000h](https://huggingface.co/hynt/Zipformer-30M-RNNT-6000h)
- Architecture: Zipformer (improved Conformer)
- Parameters: ~30M
- Training data: ~6000 gi·ªù ti·∫øng Vi·ªát
- Vocabulary: BPE ~2000 tokens

## üöÄ Quick Start

### 1. Download Model

```bash
cd /home/hiennt/VietASR/ASR
bash scripts/download_zipformer_model.sh
```

Model s·∫Ω ƒë∆∞·ª£c download v√†o: `/home/hiennt/VietASR/models/zipformer-30m-rnnt/`

### 2. Test Demo

```bash
# C√†i ƒë·∫∑t dependencies
pip install sherpa-onnx soundfile

# Test v·ªõi audio t·ª± t·∫°o
python test_demo.py

# Ho·∫∑c test v·ªõi audio file c√≥ s·∫µn
python demo_sherpa_onnx.py --audio /path/to/test.wav
```

### 3. Fine-tune Model

#### B∆∞·ªõc 1: Chu·∫©n b·ªã data

```bash
# T·∫°o Lhotse manifests
python local/prepare_custom_manifest.py \
    --corpus-dir /path/to/your/data \
    --output-dir ../data4/manifests/my_dataset \
    --dataset-name my_dataset

# Compute fbank features
python local/compute_fbank.py \
    --src-dir ../data4/manifests/my_dataset \
    --output-dir ../data4/fbank
```

#### B∆∞·ªõc 2: Convert JIT model to pretrained format

```bash
python convert_jit_to_pretrained.py \
    --jit-path ../models/zipformer-30m-rnnt/jit_script.pt \
    --output-path ../models/zipformer-30m-rnnt/pretrained.pt
```

#### B∆∞·ªõc 3: Fine-tune

```bash
bash finetune.sh \
    --pretrained-path ../models/zipformer-30m-rnnt/pretrained.pt \
    --exp-dir ../data4/exp_zipformer_finetune \
    --num-epochs 10
```

### 4. Decode v·ªõi Fine-tuned Model

```bash
bash decode.sh \
    --exp-dir ../data4/exp_zipformer_finetune \
    --epoch 10 \
    --avg 5 \
    --cuts-name test
```

### 5. Export Model

```bash
bash export.sh \
    --exp-dir ../data4/exp_zipformer_finetune \
    --output-dir ../models/zipformer-finetuned \
    --epoch 10 \
    --avg 3 \
    --export-jit 1
```

## üìÅ Scripts

| Script | M√¥ t·∫£ |
|--------|-------|
| `scripts/download_zipformer_model.sh` | Download model t·ª´ HuggingFace |
| `demo_sherpa_onnx.py` | Demo inference v·ªõi sherpa-onnx |
| `test_demo.py` | Quick test script |
| `convert_jit_to_pretrained.py` | Convert TorchScript ‚Üí PyTorch checkpoint |
| `finetune.sh` | Fine-tune model v·ªõi pretrained weights |
| `decode.sh` | Decode/transcribe audio |
| `export.sh` | Export model to TorchScript/ONNX |
| `local/prepare_custom_manifest.py` | T·∫°o Lhotse manifests |
| `local/compute_fbank.py` | Compute fbank features |

## üîß Requirements

```bash
# Core dependencies (already in Docker)
pytorch>=2.1.0
k2>=1.24.4
lhotse
kaldifeat
icefall
sentencepiece

# For demo
pip install sherpa-onnx soundfile

# For model download
pip install huggingface_hub
# or
apt-get install git-lfs
```

## üìù Notes

### V·ªÅ TorchScript vs PyTorch Checkpoint

Model t·ª´ HuggingFace ch·ªâ c√≥ `jit_script.pt` (TorchScript) - t·ªët cho inference nh∆∞ng kh√¥ng tr·ª±c ti·∫øp d√πng cho fine-tuning.

Gi·∫£i ph√°p:
1. **Option 1**: D√πng `convert_jit_to_pretrained.py` ƒë·ªÉ extract weights
2. **Option 2**: Request `pretrained.pt` t·ª´ t√°c gi·∫£ model

### Model Architecture Parameters

‚ö†Ô∏è **QUAN TR·ªåNG**: Khi fine-tune, ph·∫£i gi·ªØ nguy√™n architecture parameters:

```bash
ENCODER_DIM="384,384,384,384,384"
ENCODER_UNMASKED_DIM="256,256,256,256,256"
FEEDFORWARD_DIM="1024,1024,1024,1024,1024"
NUM_ENCODER_LAYERS="2,2,2,2,2"
NUM_HEADS="4,4,4,4,4"
DECODER_DIM=512
JOINER_DIM=512
```

### Data Format

- Audio: 16kHz, mono WAV
- Transcription: ƒë√£ normalize (lowercase, b·ªè d·∫•u c√¢u kh√¥ng c·∫ßn)
- Duration: 0.5s - 30s (recommend)

### Fine-tuning Tips

1. **Learning rate**: D√πng 10x nh·ªè h∆°n from-scratch training (~0.0003)
2. **Vocabulary**: Recommend d√πng BPE model c√≥ s·∫µn t·ª´ pretrained
3. **Data**: L·ªçc data k√©m ch·∫•t l∆∞·ª£ng v·ªõi `local/prepare_finetune_data.py`
4. **Checkpointing**: Save m·ªói 1000 steps, gi·ªØ 5 checkpoint cu·ªëi

## üê≥ Docker Usage

```bash
# Attach v√†o container
docker exec -it hiennt_vietasr_gpu bash

# Working directory
cd /vietasr/ASR

# Run scripts nh∆∞ b√¨nh th∆∞·ªùng
bash scripts/download_zipformer_model.sh
```

## ‚ùì Troubleshooting

### Demo kh√¥ng ch·∫°y

```bash
# Check dependencies
pip list | grep -E "sherpa|soundfile"

# Check model files
ls -lh ../models/zipformer-30m-rnnt/
```

### Fine-tuning OOM (Out of Memory)

Gi·∫£m `--max-duration` trong `finetune.sh`:

```bash
MAX_DURATION=150  # Thay v√¨ 300
```

### Decode ra k·∫øt qu·∫£ sai

1. Check `--use-layer-norm 0` (B·∫ÆT BU·ªòC cho model n√†y)
2. Th·ª≠ tƒÉng `--beam-size` (default: 10)
3. Check audio format (16kHz, mono)

## üìö References

- Model: https://huggingface.co/hynt/Zipformer-30M-RNNT-6000h
- Icefall: https://github.com/k2-fsa/icefall
- Sherpa-ONNX: https://github.com/k2-fsa/sherpa-onnx
