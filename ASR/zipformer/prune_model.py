import torch
import torch.nn as nn
import torch.nn.utils.prune as prune
import argparse
import os
import sentencepiece as spm # C·∫ßn import c√°i n√†y
from train import get_params, get_model, add_model_arguments

def prune_module(module, amount):
    """H√†m ti·ªán √≠ch ƒë·ªÉ c·∫Øt t·ªâa Linear layers"""
    for name, m in module.named_modules():
        if isinstance(m, nn.Linear):
            prune.l1_unstructured(m, name='weight', amount=amount)
            prune.remove(m, 'weight')

def prune_decoder_safe(model, amount=0.5):
    print(f"‚úÇÔ∏è Pruning Decoder Self-Attention: {amount}")
    if hasattr(model, 'attention_decoder') and model.attention_decoder is not None:
        for layer in model.attention_decoder.decoder_layers:
            # Ch·ªâ c·∫Øt self_attn, KH√îNG c·∫Øt src_attn hay feed_forward
            prune_module(layer.self_attn, amount)
    else:
        print("‚ö†Ô∏è Warning: Kh√¥ng t√¨m th·∫•y Attention Decoder")

def prune_encoder_last_layers(model, amount=0.5, num_last_stacks=2):
    print(f"‚úÇÔ∏è Pruning Last {num_last_stacks} Encoder Stacks: {amount}")
    encoders = model.encoder.encoders
    total_stacks = len(encoders)
    start_idx = total_stacks - num_last_stacks
    
    for i in range(start_idx, total_stacks):
        stack = encoders[i]
        print(f"   - Processing Encoder Stack {i}...")
        for layer in stack.layers:
            if hasattr(layer, 'self_attn1'):
                prune_module(layer.self_attn1, amount)
            elif hasattr(layer, 'self_attn'):
                prune_module(layer.self_attn, amount)

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--checkpoint", required=True, help="ƒê∆∞·ªùng d·∫´n file .pt g·ªëc")
    parser.add_argument("--output", required=True, help="ƒê∆∞·ªùng d·∫´n FILE l∆∞u model")
    parser.add_argument("--prune_decoder", type=float, default=0.5, help="T·ªâ l·ªá c·∫Øt Decoder Attn")
    parser.add_argument("--prune_encoder", type=float, default=0.0, help="T·ªâ l·ªá c·∫Øt Encoder cu·ªëi")
    
    # === FIX 1: Th√™m tham s·ªë bpe-model th·ªß c√¥ng ===
    parser.add_argument("--bpe-model", type=str, required=True, help="Path to BPE model")
    # ==============================================

    # N·∫°p c√°c tham s·ªë ki·∫øn tr√∫c model (encoder-dim, layers...)
    add_model_arguments(parser)

    args = parser.parse_args()

    # 1. Load Params
    params = get_params()
    params.update(vars(args))
    
    # === FIX 2: T√≠nh vocab_size t·ª´ BPE model ===
    # Model c·∫ßn bi·∫øt vocab_size ƒë·ªÉ kh·ªüi t·∫°o l·ªõp Linear cu·ªëi c√πng
    sp = spm.SentencePieceProcessor()
    sp.load(params.bpe_model)
    params.vocab_size = sp.get_piece_size()
    # C√°c token ƒë·∫∑c bi·ªát (kh·ªõp v·ªõi train.py)
    params.blank_id = sp.piece_to_id("<blk>")
    params.sos_id = params.eos_id = sp.piece_to_id("<sos/eos>")
    # ===========================================

    # C·∫•u h√¨nh c·ª©ng cho kh·ªõp v·ªõi b√†i to√°n CTC/AED
    params.use_transducer = False
    params.use_ctc = True
    params.use_attention_decoder = True
    
    # Kh·ªüi t·∫°o model
    print("üèóÔ∏è ƒêang kh·ªüi t·∫°o model...")
    model = get_model(params)
    
    # 2. Load Checkpoint
    print(f"üì• Loading checkpoint: {args.checkpoint}")
    checkpoint = torch.load(args.checkpoint, map_location="cpu")
    state_dict = checkpoint['model'] if 'model' in checkpoint else checkpoint
    model.load_state_dict(state_dict, strict=False)

    # 3. Th·ª±c hi·ªán Pruning
    if args.prune_decoder > 0:
        prune_decoder_safe(model, amount=args.prune_decoder)
        
    if args.prune_encoder > 0:
        prune_encoder_last_layers(model, amount=args.prune_encoder)

    # 4. L∆∞u Model m·ªõi
    if os.path.isdir(args.output):
        args.output = os.path.join(args.output, "pruned_model.pt")
        
    print(f"üíæ Saving pruned model to: {args.output}")
    torch.save({'model': model.state_dict()}, args.output)
    print("‚úÖ Ho√†n t·∫•t!")

if __name__ == "__main__":
    main()